\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{drf-lclcm-11,kop-rlarm-10}
\citation{emmnc-lcbbl-08,gpw-fbwrc-06,kp-pgrlf-04,tzs-spgrl-04}
\citation{phas-lgmsl-09}
\citation{gddm-fhaod-13,ksh-incdc-12,tjlb-jcngm-14}
\citation{lk-lcnnp-14}
\citation{ksh-incdc-12}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{I}{1}{Introduction}{section.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Our method learns visuomotor policies that directly use camera image observations (left) to set motor torques on a PR2 robot (right).  \relax }}{1}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:teaser}{{1}{1}{Our method learns visuomotor policies that directly use camera image observations (left) to set motor torques on a PR2 robot (right).  \relax }{figure.caption.1}{}}
\citation{kop-rlarm-10}
\citation{drf-lclcm-11,phas-lgmsl-09}
\citation{emmnc-lcbbl-08,gpw-fbwrc-06,kp-pgrlf-04,tzs-spgrl-04}
\citation{dnp-spsr-13,kbp-rlrs-13}
\citation{phas-lgmsl-09}
\citation{gddm-fhaod-13,ksh-incdc-12,tjlb-jcngm-14}
\citation{lgrn-cdbn-09}
\citation{gddm-fhaod-13}
\citation{tjlb-jcngm-14}
\citation{psgs-3ddpm-12,sl-3dgoc-07}
\citation{dnp-spsr-13}
\citation{hszg-nncss-92,p-alvin-89}
\citation{kbp-rlrs-13}
\citation{lr-avsrg-13}
\citation{rlv-arlrv-12}
\citation{gsllw-amcts-14,mksga-padrl-13}
\citation{lk-gps-13}
\citation{lk-vpsto-13,lk-lcnnp-14,mt-cbfat-14}
\citation{la-lnnpg-14,lwa-lnnpg-15}
\citation{ecr-navsr-92,mkd-vbcqp-14,whb-reecu-96}
\citation{jfn-eeuvs-97,ya-auvs-94}
\citation{ksh-incdc-12}
\citation{dnp-spsr-13}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:related}{{II}{2}{Related Work}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Overview}{2}{section.3}\protected@file@percent }
\newlabel{sec:overview}{{III}{2}{Overview}{section.3}{}}
\citation{rgb-rilsp-11}
\citation{wb-badmm-14}
\citation{lgrn-cdbn-09}
\citation{tjlb-jcngm-14}
\citation{sljsr-gdwc-14}
\citation{ddsll-lshid-09}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Visuomotor policy architecture. The network contains three convolutional layers, followed by a spatial softmax and an expected position layer that converts pixel-wise features to feature points, which are better suited for spatial computations. The points are concatenated with the robot configuration, then passed through three fully connected layers to produce the torques. \relax }}{3}{figure.caption.2}\protected@file@percent }
\newlabel{fig:nn}{{2}{3}{Visuomotor policy architecture. The network contains three convolutional layers, followed by a spatial softmax and an expected position layer that converts pixel-wise features to feature points, which are better suited for spatial computations. The points are concatenated with the robot configuration, then passed through three fully connected layers to produce the torques. \relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-A}}Network Architecture}{3}{subsection.3.1}\protected@file@percent }
\newlabel{sec:policyarch}{{\mbox  {III-A}}{3}{Network Architecture}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-B}}Training Data}{3}{subsection.3.2}\protected@file@percent }
\newlabel{sec:training}{{\mbox  {III-B}}{3}{Training Data}{subsection.3.2}{}}
\citation{lwa-lnnpg-15}
\citation{mwgcm-adopi-10}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experimental Results}{4}{section.4}\protected@file@percent }
\newlabel{sec:results}{{IV}{4}{Experimental Results}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Illustration of the tasks in our experiments, showing the variation in the position of the target for the hanger, cube, and bottle tasks, as well as two of the three grasps for the hammer, which also included variation in position (not shown).\relax }}{4}{figure.caption.5}\protected@file@percent }
\newlabel{fig:tasks}{{3}{4}{Illustration of the tasks in our experiments, showing the variation in the position of the target for the hanger, cube, and bottle tasks, as well as two of the three grasps for the hammer, which also included variation in position (not shown).\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-A}}Visuomotor Policy Generalization}{4}{subsection.4.1}\protected@file@percent }
\citation{ssp-bpcnn-03}
\citation{jsdkl-caffe-14}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Training and visual test scenes as seen by the policy at the ends of successful episodes. The hammer and bottle images were cropped for visualization only. \relax }}{5}{figure.caption.6}\protected@file@percent }
\newlabel{fig:traintest}{{4}{5}{Training and visual test scenes as seen by the policy at the ends of successful episodes. The hammer and bottle images were cropped for visualization only. \relax }{figure.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Success rates on training positions, on novel test positions, and in the presence of visual distractors. The number of trials per test is shown in parentheses.\relax }}{5}{table.caption.7}\protected@file@percent }
\newlabel{tbl:results}{{I}{5}{Success rates on training positions, on novel test positions, and in the presence of visual distractors. The number of trials per test is shown in parentheses.\relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-B}}Features Learned with End-to-End Training}{5}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Feature points learned by the shape sorting cube policy. Two of the 32 conv3 response maps are shown in (a), and the corresponding softmax distributions are displayed in (b). In (c), we show the output feature points for this input image in blue, while the feature points of the pose prediction network are shown in red. The end-to-end trained model discovers more feature points on the cube and the gripper.\relax }}{5}{figure.caption.8}\protected@file@percent }
\newlabel{fig:comppoints}{{5}{5}{Feature points learned by the shape sorting cube policy. Two of the 32 conv3 response maps are shown in (a), and the corresponding softmax distributions are displayed in (b). In (c), we show the output feature points for this input image in blue, while the feature points of the pose prediction network are shown in red. The end-to-end trained model discovers more feature points on the cube and the gripper.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-C}}CNN Architecture Evaluation}{5}{subsection.4.3}\protected@file@percent }
\newlabel{sec:poseeval}{{\mbox  {IV-C}}{5}{CNN Architecture Evaluation}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-D}}Implementation and Computational Performance}{5}{subsection.4.4}\protected@file@percent }
\citation{gddm-fhaod-13,ksh-incdc-12,tjlb-jcngm-14}
\bibstyle{plainnat}
\bibdata{references}
\bibcite{drf-lclcm-11}{{1}{2011}{{Deisenroth et~al.}}{{Deisenroth, Rasmussen, and Fox}}}
\bibcite{dnp-spsr-13}{{2}{2013}{{Deisenroth et~al.}}{{Deisenroth, Neumann, and Peters}}}
\bibcite{ddsll-lshid-09}{{3}{2009}{{Deng et~al.}}{{Deng, Dong, Socher, Li, Li, and Fei-Fei}}}
\bibcite{emmnc-lcbbl-08}{{4}{2008}{{Endo et~al.}}{{Endo, Morimoto, Matsubara, Nakanishi, and Cheng}}}
\bibcite{ecr-navsr-92}{{5}{1992}{{Espiau et~al.}}{{Espiau, Chaumette, and Rives}}}
\bibcite{gpw-fbwrc-06}{{6}{2006}{{Geng et~al.}}{{Geng, Porr, and W\"{o}rg\"{o}tter}}}
\bibcite{gddm-fhaod-13}{{7}{2014}{{Girshick et~al.}}{{Girshick, Donahue, Darrell, and Malik}}}
\bibcite{gsllw-amcts-14}{{8}{2014}{{Guo et~al.}}{{Guo, Singh, Lee, Lewis, and Wang}}}
\bibcite{hszg-nncss-92}{{9}{1992}{{Hunt et~al.}}{{Hunt, Sbarbaro, \.{Z}bikowski, and Gawthrop}}}
\bibcite{jfn-eeuvs-97}{{10}{1997}{{J\"{a}gersand et~al.}}{{J\"{a}gersand, Fuentes, and Nelson}}}
\bibcite{jsdkl-caffe-14}{{11}{2014}{{Jia et~al.}}{{Jia, Shelhamer, Donahue, Karayev, Long, Girshick, Guadarrama, and Darrell}}}
\bibcite{kop-rlarm-10}{{12}{2010}{{Kober et~al.}}{{Kober, Oztop, and Peters}}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Average pose estimation accuracy and standard deviation with various architectures, measured as average Euclidean error for the three target points in 3D, with ground truth determined by forward kinematics from the left arm. \relax }}{6}{table.caption.9}\protected@file@percent }
\newlabel{tbl:posebaseline}{{II}{6}{Average pose estimation accuracy and standard deviation with various architectures, measured as average Euclidean error for the three target points in 3D, with ground truth determined by forward kinematics from the left arm. \relax }{table.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Discussion and Future Work}{6}{section.5}\protected@file@percent }
\newlabel{sec:conclusion}{{V}{6}{Discussion and Future Work}{section.5}{}}
\bibcite{kbp-rlrs-13}{{13}{2013}{{Kober et~al.}}{{Kober, Bagnell, and Peters}}}
\bibcite{kp-pgrlf-04}{{14}{2004}{{Kohl and Stone}}{{}}}
\bibcite{ksh-incdc-12}{{15}{2012}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever, and Hinton}}}
\bibcite{lr-avsrg-13}{{16}{2013}{{Lampe and Riedmiller}}{{}}}
\bibcite{lgrn-cdbn-09}{{17}{2009}{{Lee et~al.}}{{Lee, Grosse, Ranganath, and Ng}}}
\bibcite{la-lnnpg-14}{{18}{2014}{{Levine and Abbeel}}{{}}}
\bibcite{lk-gps-13}{{19}{2013{}}{{Levine and Koltun}}{{}}}
\bibcite{lk-vpsto-13}{{20}{2013{}}{{Levine and Koltun}}{{}}}
\bibcite{lk-lcnnp-14}{{21}{2014}{{Levine and Koltun}}{{}}}
\bibcite{lwa-lnnpg-15}{{22}{2015}{{Levine et~al.}}{{Levine, Wagener, and Abbeel}}}
\bibcite{mwgcm-adopi-10}{{23}{2010}{{Meeussen et~al.}}{{Meeussen, Wise, Glaser, Chitta, McGann, Mihelich, Marder-Eppstein, Muja, Eruhimov, Foote, Hsu, Rusu, Marthi, Bradski, Konolige, Gerkey, and Berger}}}
\bibcite{mksga-padrl-13}{{24}{2013}{{Mnih et~al.}}{{Mnih, Kavukcuoglu, Silver, Graves, Antonoglou, Wierstra, and Riedmiller}}}
\bibcite{mkd-vbcqp-14}{{25}{2014}{{Mohta et~al.}}{{Mohta, Kumar, and Daniilidis}}}
\bibcite{mt-cbfat-14}{{26}{2014}{{Mordatch and Todorov}}{{}}}
\bibcite{phas-lgmsl-09}{{27}{2009}{{Pastor et~al.}}{{Pastor, Hoffmann, Asfour, and Schaal}}}
\bibcite{psgs-3ddpm-12}{{28}{2012}{{Pepik et~al.}}{{Pepik, Stark, Gehler, and Schiele}}}
\bibcite{p-alvin-89}{{29}{1989}{{Pomerleau}}{{}}}
\bibcite{rlv-arlrv-12}{{30}{2012}{{Riedmiller et~al.}}{{Riedmiller, Lange, and Voigtlaender}}}
\bibcite{rgb-rilsp-11}{{31}{2011}{{Ross et~al.}}{{Ross, Gordon, and Bagnell}}}
\bibcite{sl-3dgoc-07}{{32}{2007}{{Savarese and Fei-Fei}}{{}}}
\bibcite{ssp-bpcnn-03}{{33}{2003}{{Simard et~al.}}{{Simard, Steinkraus, and Platt}}}
\bibcite{sljsr-gdwc-14}{{34}{2014}{{Szegedy et~al.}}{{Szegedy, Liu, Jia, Sermanet, Reed, Anguelov, Erhan, Vanhoucke, and Rabinovich}}}
\bibcite{tzs-spgrl-04}{{35}{2004}{{Tedrake et~al.}}{{Tedrake, Zhang, and Seung}}}
\bibcite{tjlb-jcngm-14}{{36}{2014}{{Tompson et~al.}}{{Tompson, Jain, LeCun, and Bregler}}}
\bibcite{wb-badmm-14}{{37}{2014}{{Wang and Banerjee}}{{}}}
\bibcite{whb-reecu-96}{{38}{1996}{{Wilson et~al.}}{{Wilson, Hulls, and Bell}}}
\bibcite{ya-auvs-94}{{39}{1994}{{Yoshimi and Allen}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-A}}Feature Point Analysis}{8}{subsection.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Feature points tracked by the policy during task execution for each of the four tasks. Each feature point is displayed in a different random color, with consistent coloring across images. The policy finds features on the target object and the robot gripper and arm. In the bottle cap task, note that the policy correctly ignores the distractor bottle in the background, even though it was not present during training.\relax }}{9}{figure.caption.11}\protected@file@percent }
\newlabel{fig:ptssupp}{{6}{9}{Feature points tracked by the policy during task execution for each of the four tasks. Each feature point is displayed in a different random color, with consistent coloring across images. The policy finds features on the target object and the robot gripper and arm. In the bottle cap task, note that the policy correctly ignores the distractor bottle in the background, even though it was not present during training.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Feature points learned for each task. For each input image, the feature points produced by the policy are shown in blue, while the feature points of the pose prediction network are shown in red. The end-to-end trained policy tends to discover more feature points on the target object and the robot arm than the pose prediction network.\relax }}{9}{figure.caption.12}\protected@file@percent }
\newlabel{fig:points_comparison_supp}{{7}{9}{Feature points learned for each task. For each input image, the feature points produced by the policy are shown in blue, while the feature points of the pose prediction network are shown in red. The end-to-end trained policy tends to discover more feature points on the target object and the robot arm than the pose prediction network.\relax }{figure.caption.12}{}}
\gdef \@abspage@last{9}
