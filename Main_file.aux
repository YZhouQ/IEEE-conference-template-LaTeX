\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{drf-lclcm-11,kop-rlarm-10}
\citation{emmnc-lcbbl-08,gpw-fbwrc-06,kp-pgrlf-04,tzs-spgrl-04}
\citation{phas-lgmsl-09}
\citation{gddm-fhaod-13,ksh-incdc-12,tjlb-jcngm-14}
\citation{lk-lcnnp-14}
\citation{ksh-incdc-12}
\citation{kop-rlarm-10}
\citation{drf-lclcm-11,phas-lgmsl-09}
\citation{emmnc-lcbbl-08,gpw-fbwrc-06,kp-pgrlf-04,tzs-spgrl-04}
\citation{dnp-spsr-13,kbp-rlrs-13}
\citation{phas-lgmsl-09}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{I}{1}{Introduction}{section.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Our method learns visuomotor policies that directly use camera image observations (left) to set motor torques on a PR2 robot (right).  \relax }}{1}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:teaser}{{1}{1}{Our method learns visuomotor policies that directly use camera image observations (left) to set motor torques on a PR2 robot (right).  \relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{1}{section.2}\protected@file@percent }
\newlabel{sec:related}{{II}{1}{Related Work}{section.2}{}}
\citation{ecr-navsr-92,mkd-vbcqp-14,whb-reecu-96}
\citation{jfn-eeuvs-97,ya-auvs-94}
\citation{rgb-rilsp-11}
\citation{wb-badmm-14}
\citation{lwa-lnnpg-15}
\@writefile{toc}{\contentsline {section}{\numberline {III}Overview}{2}{section.3}\protected@file@percent }
\newlabel{sec:overview}{{III}{2}{Overview}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-A}}Network Architecture}{2}{subsection.3.1}\protected@file@percent }
\newlabel{sec:policyarch}{{\mbox  {III-A}}{2}{Network Architecture}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-B}}Training Data}{2}{subsection.3.2}\protected@file@percent }
\newlabel{sec:training}{{\mbox  {III-B}}{2}{Training Data}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experimental Results}{2}{section.4}\protected@file@percent }
\newlabel{sec:results}{{IV}{2}{Experimental Results}{section.4}{}}
\citation{mwgcm-adopi-10}
\citation{ssp-bpcnn-03}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Visuomotor policy architecture. The network contains three convolutional layers, followed by a spatial softmax and an expected position layer that converts pixel-wise features to feature points, which are better suited for spatial computations. The points are concatenated with the robot configuration, then passed through three fully connected layers to produce the torques. \relax }}{3}{figure.caption.2}\protected@file@percent }
\newlabel{fig:nn}{{2}{3}{Visuomotor policy architecture. The network contains three convolutional layers, followed by a spatial softmax and an expected position layer that converts pixel-wise features to feature points, which are better suited for spatial computations. The points are concatenated with the robot configuration, then passed through three fully connected layers to produce the torques. \relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Illustration of the tasks in our experiments, showing the variation in the position of the target for the hanger, cube, and bottle tasks, as well as two of the three grasps for the hammer, which also included variation in position (not shown).\relax }}{3}{figure.caption.5}\protected@file@percent }
\newlabel{fig:tasks}{{3}{3}{Illustration of the tasks in our experiments, showing the variation in the position of the target for the hanger, cube, and bottle tasks, as well as two of the three grasps for the hammer, which also included variation in position (not shown).\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-A}}Visuomotor Policy Generalization}{3}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Training and visual test scenes as seen by the policy at the ends of successful episodes. The hammer and bottle images were cropped for visualization only. \relax }}{3}{figure.caption.6}\protected@file@percent }
\newlabel{fig:traintest}{{4}{3}{Training and visual test scenes as seen by the policy at the ends of successful episodes. The hammer and bottle images were cropped for visualization only. \relax }{figure.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Success rates on training positions, on novel test positions, and in the presence of visual distractors. The number of trials per test is shown in parentheses.\relax }}{3}{table.caption.7}\protected@file@percent }
\newlabel{tbl:results}{{I}{3}{Success rates on training positions, on novel test positions, and in the presence of visual distractors. The number of trials per test is shown in parentheses.\relax }{table.caption.7}{}}
\citation{jsdkl-caffe-14}
\bibstyle{IEEEtran}
\bibdata{references}
\bibcite{drf-lclcm-11}{{1}{2011}{{Deisenroth et~al.}}{{Deisenroth, Rasmussen, and Fox}}}
\bibcite{dnp-spsr-13}{{2}{2013}{{Deisenroth et~al.}}{{Deisenroth, Neumann, and Peters}}}
\bibcite{emmnc-lcbbl-08}{{3}{2008}{{Endo et~al.}}{{Endo, Morimoto, Matsubara, Nakanishi, and Cheng}}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Feature points learned by the shape sorting cube policy. Two of the 32 conv3 response maps are shown in (a), and the corresponding softmax distributions are displayed in (b). In (c), we show the output feature points for this input image in blue, while the feature points of the pose prediction network are shown in red. The end-to-end trained model discovers more feature points on the cube and the gripper.\relax }}{4}{figure.caption.8}\protected@file@percent }
\newlabel{fig:comppoints}{{5}{4}{Feature points learned by the shape sorting cube policy. Two of the 32 conv3 response maps are shown in (a), and the corresponding softmax distributions are displayed in (b). In (c), we show the output feature points for this input image in blue, while the feature points of the pose prediction network are shown in red. The end-to-end trained model discovers more feature points on the cube and the gripper.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-B}}Features Learned with End-to-End Training}{4}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-C}}CNN Architecture Evaluation}{4}{subsection.4.3}\protected@file@percent }
\newlabel{sec:poseeval}{{\mbox  {IV-C}}{4}{CNN Architecture Evaluation}{subsection.4.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Average pose estimation accuracy and standard deviation with various architectures, measured as average Euclidean error for the three target points in 3D, with ground truth determined by forward kinematics from the left arm. \relax }}{4}{table.caption.9}\protected@file@percent }
\newlabel{tbl:posebaseline}{{II}{4}{Average pose estimation accuracy and standard deviation with various architectures, measured as average Euclidean error for the three target points in 3D, with ground truth determined by forward kinematics from the left arm. \relax }{table.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-D}}Implementation and Computational Performance}{4}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {V}Discussion and Future Work}{4}{section.5}\protected@file@percent }
\newlabel{sec:conclusion}{{V}{4}{Discussion and Future Work}{section.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Feature points tracked by the policy during task execution for each of the four tasks. Each feature point is displayed in a different random color, with consistent coloring across images. The policy finds features on the target object and the robot gripper and arm. In the bottle cap task, note that the policy correctly ignores the distractor bottle in the background, even though it was not present during training.\relax }}{5}{figure.caption.10}\protected@file@percent }
\newlabel{fig:ptssupp}{{6}{5}{Feature points tracked by the policy during task execution for each of the four tasks. Each feature point is displayed in a different random color, with consistent coloring across images. The policy finds features on the target object and the robot gripper and arm. In the bottle cap task, note that the policy correctly ignores the distractor bottle in the background, even though it was not present during training.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Feature points learned for each task. For each input image, the feature points produced by the policy are shown in blue, while the feature points of the pose prediction network are shown in red. The end-to-end trained policy tends to discover more feature points on the target object and the robot arm than the pose prediction network.\relax }}{5}{figure.caption.11}\protected@file@percent }
\newlabel{fig:points_comparison_supp}{{7}{5}{Feature points learned for each task. For each input image, the feature points produced by the policy are shown in blue, while the feature points of the pose prediction network are shown in red. The end-to-end trained policy tends to discover more feature points on the target object and the robot arm than the pose prediction network.\relax }{figure.caption.11}{}}
\bibcite{ecr-navsr-92}{{4}{1992}{{Espiau et~al.}}{{Espiau, Chaumette, and Rives}}}
\bibcite{gpw-fbwrc-06}{{5}{2006}{{Geng et~al.}}{{Geng, Porr, and W\"{o}rg\"{o}tter}}}
\bibcite{gddm-fhaod-13}{{6}{2014}{{Girshick et~al.}}{{Girshick, Donahue, Darrell, and Malik}}}
\bibcite{jfn-eeuvs-97}{{7}{1997}{{J\"{a}gersand et~al.}}{{J\"{a}gersand, Fuentes, and Nelson}}}
\bibcite{jsdkl-caffe-14}{{8}{2014}{{Jia et~al.}}{{Jia, Shelhamer, Donahue, Karayev, Long, Girshick, Guadarrama, and Darrell}}}
\bibcite{kop-rlarm-10}{{9}{2010}{{Kober et~al.}}{{Kober, Oztop, and Peters}}}
\bibcite{kbp-rlrs-13}{{10}{2013}{{Kober et~al.}}{{Kober, Bagnell, and Peters}}}
\bibcite{kp-pgrlf-04}{{11}{2004}{{Kohl and Stone}}{{}}}
\bibcite{ksh-incdc-12}{{12}{2012}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever, and Hinton}}}
\bibcite{lk-lcnnp-14}{{13}{2014}{{Levine and Koltun}}{{}}}
\bibcite{lwa-lnnpg-15}{{14}{2015}{{Levine et~al.}}{{Levine, Wagener, and Abbeel}}}
\bibcite{mwgcm-adopi-10}{{15}{2010}{{Meeussen et~al.}}{{Meeussen, Wise, Glaser, Chitta, McGann, Mihelich, Marder-Eppstein, Muja, Eruhimov, Foote, Hsu, Rusu, Marthi, Bradski, Konolige, Gerkey, and Berger}}}
\bibcite{mkd-vbcqp-14}{{16}{2014}{{Mohta et~al.}}{{Mohta, Kumar, and Daniilidis}}}
\bibcite{phas-lgmsl-09}{{17}{2009}{{Pastor et~al.}}{{Pastor, Hoffmann, Asfour, and Schaal}}}
\bibcite{rgb-rilsp-11}{{18}{2011}{{Ross et~al.}}{{Ross, Gordon, and Bagnell}}}
\bibcite{ssp-bpcnn-03}{{19}{2003}{{Simard et~al.}}{{Simard, Steinkraus, and Platt}}}
\bibcite{tzs-spgrl-04}{{20}{2004}{{Tedrake et~al.}}{{Tedrake, Zhang, and Seung}}}
\bibcite{tjlb-jcngm-14}{{21}{2014}{{Tompson et~al.}}{{Tompson, Jain, LeCun, and Bregler}}}
\bibcite{wb-badmm-14}{{22}{2014}{{Wang and Banerjee}}{{}}}
\bibcite{whb-reecu-96}{{23}{1996}{{Wilson et~al.}}{{Wilson, Hulls, and Bell}}}
\bibcite{ya-auvs-94}{{24}{1994}{{Yoshimi and Allen}}{{}}}
\gdef \@abspage@last{6}
